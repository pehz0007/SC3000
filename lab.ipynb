{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Install Dependencies",
   "id": "da648cb7a77db45d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "!pip install gymnasium\n",
    "!pip install gymnasium[classic-control]\n",
    "!pip install gymnasium[other]\n",
    "!pip install IPython"
   ],
   "id": "6079517bfc468213"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Utilities Functions from the Lab sheets",
   "id": "598cb450f959421b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "from IPython import display as ipythondisplay\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def show_result(episode_results, title=''):\n",
    "    plt.plot(episode_results)\n",
    "    plt.title('Cumulative reward for each episode ({})'.format(title))\n",
    "    plt.ylabel('Cumulative reward')\n",
    "    plt.xlabel('episode')\n",
    "    plt.show()\n",
    "\n",
    "def show_video_html():\n",
    "  mp4list = glob.glob('videos/*.mp4')\n",
    "  if len(mp4list) > 0:\n",
    "    mp4 = mp4list[0]\n",
    "    video = io.open(mp4, 'r+b').read()\n",
    "    encoded = base64.b64encode(video).decode('ascii')\n",
    "    ipythondisplay.display(HTML(data='<video controls src=\"data:video/x-m4v;base64,{0}\">'.format(encoded)))\n",
    "  else:\n",
    "    print(\"Could not find video\")"
   ],
   "id": "cd225859d76b846"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## `Agent` Class\n",
    "Defining a simple `Agent` Interface which consist of:\n",
    "\n",
    "1. `step(action_space, observation)` - A single timestep where the agent can decide which action to take based on the current observation. The action return by the function will be executed by the environment.\n",
    "\n",
    "2. `reward(immediate_reward, prev_action, prev_observation, new_observation)` - Updates the agent based on the immediate reward after performing the action given from `step`\n",
    "\n",
    "3. `reset()` - Called when the episode ends and to reset the agent if needed"
   ],
   "id": "d11f2dd11f73559d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class Agent:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.cumulative_reward = 0\n",
    "\n",
    "    def step(self, action_space, observation):\n",
    "        pass\n",
    "\n",
    "    def reward(self, immediate_reward, prev_action, prev_observation, new_observation):\n",
    "        pass\n",
    "\n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "def run_episode(env, agent: Agent, show_video=False, evaluation = False):\n",
    "    observation, info = env.reset()\n",
    "    cumulative_reward = 0\n",
    "    while True:\n",
    "        action = agent.step(env.action_space, observation)\n",
    "        new_observation, reward, terminated, truncated, info  = env.step(action)\n",
    "        if terminated:\n",
    "            reward = -1\n",
    "        cumulative_reward += reward\n",
    "        if not evaluation:\n",
    "            agent.reward(reward, action, observation, new_observation)\n",
    "        observation = new_observation\n",
    "        if terminated or truncated:\n",
    "            observation, info = env.reset()\n",
    "            print(\"Cumulative reward for this round:\", cumulative_reward)\n",
    "            agent.reset()\n",
    "            break\n",
    "    if show_video:\n",
    "        show_video_html()\n",
    "    return cumulative_reward\n",
    "\n"
   ],
   "id": "52411bcd79205bcc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## `RandomAgent` class\n",
    "\n",
    "Here we define a `RandomAgent` by implementing `Agent` class\n",
    "## Overridden methods:\n",
    "1. `step` - Sample an action from the `action_space`\n",
    "\n"
   ],
   "id": "dc31b20d34aa5d55"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class RandomAgent(Agent):\n",
    "    def step(self, action_space, observation):\n",
    "        return action_space.sample()\n"
   ],
   "id": "7190c99c5e628102"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Run the `RandomAgent` with 100 episodes\n",
    "\n",
    "We can see that the cumulative reward is not increasing as expect from a `RandomAgent`"
   ],
   "id": "e6296e76b7a0f6b0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "episode_results = []\n",
    "env = RecordVideo(gym.make(\"CartPole-v1\", render_mode=\"rgb_array\"), \"./videos\")\n",
    "random_agent = RandomAgent()\n",
    "for i in range(100):\n",
    "    episode_results.append(run_episode(env, random_agent))"
   ],
   "id": "ae1a1ca7db59bfdb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **Value-Based** and **Policy-Based** methods\n",
    "In Reinforcement Learning, RL algorithms are categorise into **Value-Based** and **Policy-Based** methods.\n",
    "\n",
    "In **Value-Based** method, we parameterise the value function defined by the Bellman Equations and we try to find the optimal value function that will give us the optimal policy.\n",
    "\n",
    "Example of **Value-Based** methods: Value Iterations, Policy Iteration (because the policy is defined by the state-action value function), SARSA (On-policy), Q-Learning (Off-Policy)\n",
    "\n",
    "In **Policy-Based** method, we parameterise the policy directly instead and improve the policy directly without using the value function.\n",
    "\n",
    "Example of **Policy-Based** methods: REINFORCE, Actor-Critic, PPO\n",
    "\n",
    "We will be testing algorithms from both methods to learn from the environment. This exploration will help us understand the strengths and weaknesses of each approach and their applicability to different types of RL problems."
   ],
   "id": "63c5bd52d7c85a11"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Value-Based",
   "id": "5e915078d7b39726"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## `QLearningControlAgent` Class\n",
    "\n",
    "## Defining Q Learning (Off Policy Learning)\n",
    "\n",
    "In Q Learning we do not need access to the MDP, no importance sampling and we do not need to wait for the episode to terminate for the Agent to start learning *See TD learning*.\n",
    "\n",
    "### Formulation for Q Learning:\n",
    "\n",
    "#### Policy (ε-greedy):\n",
    "\n",
    "$\n",
    "\\pi(a \\mid s) = \n",
    "\\begin{cases} \n",
    "\\arg\\max_{a} Q(s, a) & \\text{with probability } (1 - \\epsilon), \\\\\n",
    "\\text{a random action} & \\text{with probability } \\epsilon.\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "#### Update Rule:\n",
    "\n",
    "$Q(S_{t},A_{t}) \\gets Q(S_{t},A_{t}) + \\alpha[R_{t+1} + \\gamma max_{a}Q(S_{s+1},a)-Q(S_{t},A_{t})]$\n",
    "\n",
    "\n",
    "**Important Note**: Q Learning is off policy because when we update the state action function we update toward:\n",
    "\n",
    "$max_{a}Q(S_{s+1},a)$ \n",
    "\n",
    "Which is not always going to be the chosen action by the policy during training. Instead the ε-greedy policy may give choose a different action which makes the update rule independent from the policy itself.\n",
    "\n",
    "Our learning rate and epsilon will have a log decay with a mininum lr and epsilon where $t$ is the episode.\n",
    "\n",
    "$\\text{learning\\_rate} = \\max \\left( \\text{min\\_lr}, \\min \\left( 1, 1 - \\log_{10} \\left( \\frac{t + 1}{\\text{decay}} \\right) \\right) \\right)$\n",
    "\n",
    "$\\text{epsilon} = \\max \\left( \\text{min\\_epsilon}, \\min \\left( 1, 1 - \\log_{10} \\left( \\frac{t + 1}{\\text{decay}} \\right) \\right) \\right)$\n",
    "\n",
    "## Overridden methods:\n",
    "1. `step` - Select an action based on ε-greedy.\n",
    "2. `reward` - Update the Q Table by using the update rule.\n"
   ],
   "id": "984c6c68f771e82b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "\n",
    "# Model free\n",
    "class QLearningControlAgent(Agent):\n",
    "\n",
    "    def __init__(self,\n",
    "        lower_bounds,\n",
    "        upper_bounds,\n",
    "        num_bins,\n",
    "        num_action,\n",
    "        min_epsilon=0.001, discount_factor=0.9, min_lr=0.1, decay=20,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.discount_factor = discount_factor\n",
    "        self.min_lr = min_lr\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.decay = decay\n",
    "        self.t = 0\n",
    "        self.lower_bounds = lower_bounds\n",
    "        self.upper_bounds = upper_bounds\n",
    "        self.num_bins = np.array(num_bins)\n",
    "        self.qtable = np.zeros(np.append(self.num_bins + 1, num_action))\n",
    "\n",
    "    def to_discrete_state(self, observation):\n",
    "        '''\n",
    "        We need to convert the continuous observation into discrete state by bining the values\n",
    "        '''\n",
    "        bin_window = (self.upper_bounds - self.lower_bounds) / self.num_bins\n",
    "        clamped_observation = np.clip(observation, self.lower_bounds, self.upper_bounds)\n",
    "        discrete_state = np.floor((clamped_observation - self.lower_bounds) / bin_window).astype(np.int32)\n",
    "        return tuple(discrete_state)\n",
    "\n",
    "    def get_learning_rate(self):\n",
    "        return max(self.min_lr, min(1., 1. - np.log10((self.t + 1) / self.decay)))\n",
    "\n",
    "    def get_epsilon(self):\n",
    "        return max(self.min_epsilon, min(1., 1. - np.log10((self.t + 1) / self.decay)))\n",
    "\n",
    "    def step(self, action_space, observation):\n",
    "        state = self.to_discrete_state(observation)\n",
    "        # Epsilon-greedy policy\n",
    "        if np.random.random() < self.get_epsilon():\n",
    "            action = np.random.randint(0, action_space.n)  # Random action\n",
    "        else:\n",
    "            action = np.argmax(self.qtable[state])  # Greedy action\n",
    "        return action\n",
    "\n",
    "    def reward(self, immediate_reward, prev_action, prev_observation, new_observation):\n",
    "        prev_state = self.to_discrete_state(prev_observation)\n",
    "        new_state = self.to_discrete_state(new_observation)\n",
    "        self.qtable[prev_state + (prev_action,)] += self.get_learning_rate() * (immediate_reward + self.discount_factor * np.max(self.qtable[new_state]) - self.qtable[prev_state + (prev_action,)])\n",
    "\n",
    "    def reset(self):\n",
    "        self.t += 1\n",
    "\n",
    "episode_results = []\n",
    "env = RecordVideo(gym.make(\"CartPole-v1\", render_mode=\"rgb_array\"), \"./videos\")\n",
    "upper_bounds = np.array([env.observation_space.high[0], 0.5, env.observation_space.high[2], np.radians(50) / 1.]) # Upper bounds for our bining\n",
    "lower_bounds = np.array([env.observation_space.low[0], -0.5, env.observation_space.low[2], - np.radians(50) / 1.]) # Lower bounds for our bining\n",
    "qlearning_agent = QLearningControlAgent(lower_bounds, upper_bounds, [3, 3, 6, 6], env.action_space.n)\n",
    "for i in range(1000):\n",
    "    print(\"Round: \" + str(i))\n",
    "    episode_results.append(run_episode(env, qlearning_agent))\n",
    "\n",
    "show_result(episode_results)"
   ],
   "id": "4474e6a48667f9ea"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<H4> Imports for creating a Deep Q learning agent",
   "id": "f5a7655671ee72b6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import os\n",
    "from collections import deque"
   ],
   "id": "6881b82f63e93029"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Here we build a simple feed forward neural network using 512 neurons on first layer followed by 256 on the second layer, 64 on the third layer and finally the output layer.",
   "id": "8249103be5e174dc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(state_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, action_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ],
   "id": "6b23f3e33d8e4b0a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## `DeepQLearningAgent` Class\n",
    "\n",
    "Defining Deep Q Learning (Off Policy Learning)\n",
    "\n",
    "Deep Q Learning is an extension of Q Learning which replaces the Q Table with a deep neural network as a function approximator for the state-action function. DQN also uses a Replay Buffer to store past experiences.\n",
    "\n",
    "Formulation for Deep Q Learning is similar to the Q Learning but the state-action function is now parameterise by $\\theta$:\n",
    "\n",
    "Policy (ε-greedy):\n",
    "\n",
    "$\n",
    "\\pi(a \\mid s) = \n",
    "\\begin{cases} \n",
    "\\arg\\max_{a} Q_{\\theta}(s, a) & \\text{with probability } (1 - \\epsilon), \\\\\n",
    "\\text{a random action} & \\text{with probability } \\epsilon.\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "Update Rule:\n",
    "\n",
    "$target \\gets [R_{t+1} + \\gamma max_{a}Q_{\\theta, target\\ network}(S_{s+1},a)]$\n",
    "\n",
    "$\\theta \\leftarrow \\theta - \\alpha \\nabla_{\\theta} \\left( \\text{target} - Q_{\\theta}(S_{t}, A_{t}) \\right)^2$\n",
    "\n",
    "*Note*: We will be using Adam Optimizer instead of a simple gradient descent\n",
    "\n",
    "Overridden methods:\n",
    "\n",
    "`step` - Select an action based on ε-greedy.\n",
    "\n",
    "`reward` - Append the rewards, actions and observations to the Replay Buffer\n",
    "\n",
    "`reset` - Update the nerual network from the Replay Buffer by using the update rule."
   ],
   "id": "36e94bb786c4d9c2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class DeepQLearningAgent(Agent):\n",
    "    def __init__(self, state_size, action_size, min_epsilon=0.001, discount_factor=0.9, lr=0.0003, decay=3):\n",
    "        super().__init__()\n",
    "\n",
    "        # check for gpu accelerator\n",
    "        self.device = torch.device(\n",
    "            \"cuda\" if torch.cuda.is_available() else\n",
    "            \"mps\" if torch.backends.mps.is_available() else\n",
    "            \"cpu\"\n",
    "        )\n",
    "\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.discount_factor = discount_factor\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.lr = lr\n",
    "        self.decay = decay\n",
    "        self.t = 0\n",
    "        self.epsilon = 1.0\n",
    "\n",
    "        # here the agent keeps the previous experience\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.batch_size = 64\n",
    "\n",
    "        # the Q network will learn and update frequently from experience\n",
    "        # while the target network will only update after a certain number of episodes\n",
    "        self.q_network = NeuralNetwork(state_size, action_size).to(self.device)\n",
    "        self.target_network = NeuralNetwork(state_size, action_size).to(self.device)\n",
    "        self.update_target_network()\n",
    "\n",
    "        self.optimizer = optim.AdamW(self.q_network.parameters(), lr=self.lr, amsgrad=True)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "\n",
    "    def get_epsilon(self):\n",
    "        self.epsilon = max(self.min_epsilon, min(1., 1. - np.log10((self.t + 1) / self.decay)))\n",
    "        return self.epsilon\n",
    "\n",
    "    # here the agent either explore by choosing a random action\n",
    "    # or exploit by choosing an action which give the highest Q value from the neural network\n",
    "    def step(self, action_space, observation):\n",
    "        if np.random.random() < self.get_epsilon():\n",
    "            return np.random.randint(0, self.action_size)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                observation = torch.tensor(observation, dtype=torch.float32, device=self.device)\n",
    "                q_values = self.q_network(observation)\n",
    "                return torch.argmax(q_values).item()\n",
    "\n",
    "    # here when my agent interact with the cart pole environment it will take in a reward and new observation\n",
    "    def reward(self, immediate_reward, prev_action, prev_observation, new_observation):\n",
    "        self.memory.append((prev_observation, prev_action, immediate_reward, new_observation))\n",
    "        if len(self.memory) >= 1000 and self.epsilon != 0.0:\n",
    "            self.replay_experience()\n",
    "\n",
    "    def replay_experience(self):\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        observation, actions, rewards, next_observation = zip(*batch)\n",
    "\n",
    "        observation = torch.tensor(observation, dtype=torch.float32, device=self.device)\n",
    "        actions = torch.tensor(actions, dtype=torch.long, device=self.device).unsqueeze(dim=1)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32, device=self.device).unsqueeze(dim=1)\n",
    "        next_observation = torch.tensor(next_observation, dtype=torch.float32, device=self.device)\n",
    "\n",
    "        current_q = self.q_network(observation).gather(1, actions)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.q_network(next_observation)\n",
    "            next_actions = torch.argmax(next_q_values, dim=1, keepdim=True)\n",
    "            max_next_q = self.target_network(next_observation).gather(1, next_actions)\n",
    "\n",
    "        target_q = rewards + (self.discount_factor * max_next_q)\n",
    "\n",
    "        loss = self.loss_fn(current_q, target_q)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    # here the function is called start of every episode\n",
    "    # % 10 ensure target network is updated every 10 episodes to prevent instability to training\n",
    "    def reset(self):\n",
    "        self.t += 1\n",
    "        if self.t % 10 == 0:\n",
    "            self.update_target_network()\n",
    "\n",
    "    # here we can save the model we just trained\n",
    "    def save_model(self, file_path=\"models/deep_q_learning_agent.pth\"):\n",
    "        os.makedirs(os.path.dirname(file_path), exist_ok=True)  # Create directory if it doesn't exist\n",
    "        torch.save(self.q_network.state_dict(), file_path)\n",
    "        print(f\"Model saved to {file_path}\")\n",
    "\n",
    "    # here we can load the saved model for evaluation\n",
    "    def load_model(self, file_path=\"models/deep_q_learning_agent.pth\"):\n",
    "        if os.path.exists(file_path):\n",
    "            self.q_network.load_state_dict(torch.load(file_path))\n",
    "            self.update_target_network()  # Ensure target network is also updated\n",
    "            print(f\"Model loaded from {file_path}\")\n",
    "        else:\n",
    "            print(f\"Model file '{file_path}' does not exist.\")\n",
    "\n",
    "    # set the agent to go for exploitation only without exploration\n",
    "    def set_evaluation_mode(self):\n",
    "        self.epsilon = 0.0"
   ],
   "id": "2d4968c24de36a53"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<H4> Here we initialise the environement and the Deep Q learning agent",
   "id": "3b241559700e33"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "episode_results = []\n",
    "env = RecordVideo(gym.make(\"CartPole-v1\", render_mode=\"rgb_array\"), \"./videos\")\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "deep_q_learning_agent = DeepQLearningAgent(state_size, action_size)"
   ],
   "id": "663f006b33265ea2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<H4> Here we train the agent, we will train the agent for 500 episode and saving the model at the end",
   "id": "9cf08a2c77807c8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for i in range(500):\n",
    "    cumulative_reward = run_episode(env, deep_q_learning_agent)\n",
    "    episode_results.append(cumulative_reward)\n",
    "\n",
    "    print(f\"Episode {i+1}: Cumulative Reward = {cumulative_reward}\")\n",
    "\n",
    "show_result(episode_results)\n",
    "\n",
    "deep_q_learning_agent.save_model(\"models/deep_q_learning_agent.pth\")\n"
   ],
   "id": "de1b67b93dcd2ac7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Policy-Based",
   "id": "f6e96c08949a312e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In **Policy-Based** method, we are trying to improve our policy directly.\n",
    "\n",
    "Most **Policy-Based** method uses policy gradient techniques to improve the policy and **Policy Gradient Theorem** is central to this approach:\n",
    "\n",
    "### **Policy Gradient Theorem**\n",
    "\n",
    "The **Policy Gradient Theorem** provides a way to estimate the gradient of the expected cumulative reward $J(\\theta)$ with respect to the policy parameters $\\theta$.\n",
    "\n",
    "$\n",
    "\\nabla_{\\theta} J(\\theta) = \\mathbb{E}_{\\pi_{\\theta}} [ \\nabla_{\\theta} \\log \\pi_{\\theta}(a | s) \\cdot Q^{\\pi_{\\theta}}(s, a)]\n",
    "$\n",
    "\n",
    "Where $\\nabla_{\\theta} \\log \\pi_{\\theta}(a | s)$ is the score function.\n",
    "\n",
    "and $Q^{\\pi_{\\theta}}(s, a)$ is the long term reward following the policy parameterise by $\\theta$."
   ],
   "id": "c1aa429a937d264a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## `REINFORCEAgent` Class\n",
    "\n",
    "Defining Monte-Carlo Policy Gradient (REINFORCE)\n",
    "\n",
    "REINFORCE is a simple **Policy-Based** method which use the **Policy Gradient Theorem** to improve the policy. The algorithm will **maximize the cumulative reward** from the policy objective function using gradient ascent.\n",
    "\n",
    "Policy (Softmax Policy):\n",
    "\n",
    "$\n",
    "\\pi_{\\theta}(a | s) = \\frac{e^{f_{\\theta}(s, a)}}{\\sum_{a'} e^{f_{\\theta}(s, a')}}\n",
    "$\n",
    "\n",
    "Update Rule:\n",
    "\n",
    "$\\nabla_{\\theta} J(\\theta) = \\mathbb{E}_{\\pi_{\\theta}} \\left[ \\sum_{t=0}^{T} \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t | s_t) G_t \\right]$\n",
    "\n",
    "$\\theta \\leftarrow \\theta + \\alpha \\left( \\nabla_{\\theta} J(\\theta) \\right)$\n",
    "\n",
    "*Note*: We will be using Adam Optimizer instead of a simple gradient ascent\n",
    "\n",
    "Overridden methods:\n",
    "\n",
    "`step` - Select an action based on Softmax Policy. \n",
    "\n",
    "`reward` - Append the rewards, actions and observations\n",
    "\n",
    "`reset` - Update the policy after the episode ends using the update rule and clear the appended rewards, actions and observations\n"
   ],
   "id": "d83144de2be867d8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class REINFORCEAgent(Agent):\n",
    "    def __init__(self, state_size, action_size, discount_factor=0.9, lr=0.0003):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = torch.device(\n",
    "            \"cuda\" if torch.cuda.is_available() else\n",
    "            \"mps\" if torch.backends.mps.is_available() else\n",
    "            \"cpu\"\n",
    "        )\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.episodes = []\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.discount_factor = discount_factor\n",
    "        self.lr = lr\n",
    "\n",
    "        self.policy = NeuralNetwork(state_size, action_size).to(self.device)\n",
    "        self.optimizer = optim.AdamW(self.policy.parameters(), lr=self.lr, amsgrad=True)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "    def discount_rewards(self, rewards, gamma):\n",
    "        discounted = []\n",
    "        cumulative_reward = 0.0\n",
    "        for reward in reversed(rewards):\n",
    "            cumulative_reward = reward + gamma * cumulative_reward\n",
    "            discounted.insert(0, cumulative_reward)\n",
    "        return discounted\n",
    "\n",
    "    def step(self, action_space, observation):\n",
    "        observation = torch.tensor(observation, dtype=torch.float32, device=self.device)\n",
    "        action_probs = F.softmax(self.policy(observation), dim=0)\n",
    "        action_distribution = torch.distributions.Categorical(action_probs)\n",
    "        action = action_distribution.sample()\n",
    "        return action.item()\n",
    "    \n",
    "    def reward(self, immediate_reward, prev_action, prev_observation, new_observation):\n",
    "        self.episodes.append((prev_observation, prev_action, immediate_reward))\n",
    "\n",
    "    def update(self):\n",
    "        discounted_rewards = self.discount_rewards(list(zip(*self.episodes))[2], self.discount_factor)\n",
    "        discounted_rewards = torch.tensor(discounted_rewards, dtype=torch.float32, device=self.device)\n",
    "\n",
    "        discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-8)\n",
    "\n",
    "        policy_loss = []\n",
    "        for (observation, action, immediate_reward), discounted_reward in zip(self.episodes, discounted_rewards):\n",
    "            observation = torch.tensor(observation, dtype=torch.float32, device=self.device)\n",
    "            action_probs = F.softmax(self.policy(observation), dim=0)\n",
    "            log_prob = torch.log(action_probs[action])\n",
    "            policy_loss.append(-log_prob * discounted_reward) # Gradient Ascent\n",
    "\n",
    "        policy_loss = torch.stack(policy_loss).sum()\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.episodes = []\n",
    "\n",
    "    def reset(self):\n",
    "        self.update()\n",
    "\n",
    "    def save_model(self, file_path=\"models/reinforce_learning_agent.pth\"):\n",
    "        os.makedirs(os.path.dirname(file_path), exist_ok=True)  # Create directory if it doesn't exist\n",
    "        torch.save(self.policy.state_dict(), file_path)\n",
    "        print(f\"Model saved to {file_path}\")\n",
    "\n",
    "    def load_model(self, file_path=\"models/reinforce_learning_agent.pth\"):\n",
    "        if os.path.exists(file_path):\n",
    "            self.policy.load_state_dict(torch.load(file_path))\n",
    "            print(f\"Model loaded from {file_path}\")\n",
    "        else:\n",
    "            print(f\"Model file '{file_path}' does not exist.\")"
   ],
   "id": "631ce99c0c9393e9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "episode_results = []\n",
    "env = RecordVideo(gym.make(\"CartPole-v1\", render_mode=\"rgb_array\"), \"./videos\")\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "reinforce_agent = REINFORCEAgent(state_size, action_size)\n",
    "\n",
    "for i in range(500):\n",
    "    cumulative_reward = run_episode(env, reinforce_agent)\n",
    "    episode_results.append(cumulative_reward)\n",
    "\n",
    "show_result(episode_results)\n",
    "\n",
    "reinforce_agent.save_model()"
   ],
   "id": "2a98cc9876da7163"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Task 1",
   "id": "414cdacfe7d44a8f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Here we sample a random state of the cart pole enviroment to input into our current agent which will output a action to check if our action is correct, we also show the image of the current state to better visualise",
   "id": "49961f92b29ca666"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "deep_q_learning_agent.load_model(\"models/deep_q_learning_agent.pth\")\n",
    "deep_q_learning_agent.set_evaluation_mode()\n",
    "\n",
    "observation, info = env.reset()\n",
    "image = env.render()\n",
    "\n",
    "action = deep_q_learning_agent.step(env.action_space, observation)\n",
    "print(\"Observation: \" + str(observation))\n",
    "print(\"Action: \" + str(action))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ],
   "id": "c1d83d714b0293"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<H2> Task 2",
   "id": "edfb567c8b8ba00f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Here we load our previously saved 3 models for evaluation\n",
    "- Q Learning\n",
    "- Deep Q Learning\n",
    "- REINFORCE Learning"
   ],
   "id": "d430fbd64245dad4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Evaluation for Q Learning Agent\n",
    "evaluation_results = []\n",
    "for i in range(100):\n",
    "    cumulative_reward = run_episode(env, qlearning_agent, evaluation=True)\n",
    "    evaluation_results.append(cumulative_reward)\n",
    "    print(f\"Evaluation Episode {i+1}: Reward = {cumulative_reward}\")\n",
    "\n",
    "print(\"Average cumulative reward: \" + str(np.mean(evaluation_results)))\n",
    "print(\"Is my agent good enough: \" + str(np.mean(evaluation_results) > 195))\n",
    "show_result(evaluation_results, \"Q Learning Agent\")\n",
    "\n",
    "\n",
    "# Evaluation for Deep Q Learning Agent\n",
    "deep_q_learning_agent.load_model(\"models/deep_q_learning_agent.pth\")\n",
    "deep_q_learning_agent.set_evaluation_mode()\n",
    "\n",
    "evaluation_results = []\n",
    "for i in range(100):\n",
    "    cumulative_reward = run_episode(env, deep_q_learning_agent, evaluation=True)\n",
    "    evaluation_results.append(cumulative_reward)\n",
    "    print(f\"Evaluation Episode {i+1}: Reward = {cumulative_reward}\")\n",
    "\n",
    "print(\"Average cumulative reward: \" + str(np.mean(evaluation_results)))\n",
    "print(\"Is my agent good enough: \" + str(np.mean(evaluation_results) > 195))\n",
    "show_result(evaluation_results, \"Deep Q Learning Agent\")\n",
    "\n",
    "\n",
    "# Evaluation for REINFORCE Agent\n",
    "reinforce_agent.load_model(\"models/reinforce_learning_agent.pth\")\n",
    "evaluation_results = []\n",
    "for i in range(100):\n",
    "    cumulative_reward = run_episode(env, reinforce_agent, evaluation=True)\n",
    "    evaluation_results.append(cumulative_reward)\n",
    "    print(f\"Evaluation Episode {i+1}: Reward = {cumulative_reward}\")\n",
    "\n",
    "print(\"Average cumulative reward: \" + str(np.mean(evaluation_results)))\n",
    "print(\"Is my agent good enough: \" + str(np.mean(evaluation_results) > 195))\n",
    "show_result(evaluation_results, \"REINFORCE Agent\")"
   ],
   "id": "8e6e2483448ea162"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<H2> Task 3",
   "id": "5a89d95e7322be2f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<H4> Here we will render 1 episode played by the agent and output the cumlative reward",
   "id": "e1366e2adb2aadbf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sympy import true\n",
    "\n",
    "deep_q_learning_agent.load_model(\"models/deep_q_learning_agent.pth\")\n",
    "deep_q_learning_agent.set_evaluation_mode()\n",
    "\n",
    "env = RecordVideo(gym.make(\"CartPole-v1\", render_mode=\"rgb_array\"), \"./videos\")\n",
    "run_episode(env, deep_q_learning_agent, show_video=true)\n",
    "env.close()"
   ],
   "id": "508d10a02d21a895"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Conclusion\n",
    "\n",
    "In this assignment, we implemented three different RL algorithms; Q Learning, Deep Q Learning and REINFORCE. "
   ],
   "id": "afa822069f2dbf0d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "424ef9aba34a834b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
